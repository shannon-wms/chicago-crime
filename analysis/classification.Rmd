---
title: "Arrest Classification"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = paste0(rprojroot::find_rstudio_root_file(), "/chigcrim"))
```

## Imports
```{r message=FALSE}
# Load required libraries
library(tidyverse)
library(lubridate)
library(randomForest)
library(e1071)
library(xtable)

# Load package functions
devtools::load_all("../chigcrim/")

# Use seed to ensure reproducibility
set.seed(1)
```


## Introduction
In this section, the binary classification task of predicting if a suspect is arrested is considered. Three models will be investigated, logistic regression, support vector machines and random forest classifiers.

## Assessing performance
To assess performance, three metrics will be used, the overall accuracy, the sensitivity and the specificity. These are defined as follows:

- Overall accuracy: The proportion of correct predictions.

- Sensitivity: Proportion of positive observations correctly predicted.

- Specificity: Proportion of negative observations correctly predicted.

Note that as logistic regression is a probabilistic classifier, the results are rounded to yield predictions that can be assessed with the metrics above to facilitate the comparison with the support vector machine results.

```{r}
# List of metrics used to evaluate classification models
# These are used by cv_eval function
metrics <- list(accuracy = classification_accuracy,
                sensitivity = classification_sensitivity,
                specificity = classification_specificity)
```

For each classifier, we use repeated k-fold cross validation. Specifically, 5-fold cross validation is repeated three times (run in parallel), and the metrics are averaged across the folds.

## Feature selection
Due to computational limitations, only the 2019 data will be considered. Below, the feature selection choices are outlined. These decisions involved incorporating information from the exploratory data analysis (EDA), as well as considering what is computationally feasible.

- Encode the date as the day in the year (1-365).

- Encode time of day as a float 0-24 (this looks good in the EDA).

- Drop `year` as these are all 2019.

- We will drop the `id`s, as it contains unique values.

- We drop `case_number`, as it contains almost all unique values.

- Drop `primary_type`, `description` and `iucr` code. Keep `fbi_code` as an indicator of crime type.

- Assume `updated_on` is not informative (see EDA).

- `latitude` and `longitude` are dropped. `x_coordinates` and `y_coordinates` are kept. Note that these coordinates likely not particularly useful for linear classifiers, but should be useful for non-linear methods.

- Community areas will be kept, but other areas (`district`, `beat`, `ward` and `block`) are dropped.

- NAs will be dropped (see EDA).

- Particularly rare factors will be grouped into a variable other (see `?otherise`).



```{r, cache = TRUE}
# Load 2019 data to build classification models
df <- load_data(year = 2019, strings_as_factors = FALSE)

# Convert rareuly used fbi categories to other
df$fbi_code <- otherise(df$fbi_code, 500)
df$location_description <- otherise(df$location_description, 1000)

# List of features for removal, EDA showed not necessary
remove_features <- c("id", "year", "case_number", "primary_type", "description",
                     "iucr", "updated_on", "latitude", "longitude",
                     "date", "district", "beat", "ward", "block")

# Add more time features
df <- df %>%
  mutate(day = yday(df$date),
         time = hour(df$date) + minute(df$date)/60) %>%
  select(-all_of(remove_features)) %>%
  filter(complete.cases(df))

# Convert to factors
df <- df %>%
  mutate(location_description = as.factor(location_description),
         fbi_code = as.factor(fbi_code),
         community_area = as.factor(community_area))

head(df)
```

### Logistic Regression
We will first consider a logistic regression classifier. This is a linear classifier. It calls the function `optim()` internally to minimize the cross-entropy/log-loss function using gradient based optimisation methods (here BFGS).
```{r time_it=TRUE, echo = T, eval = F}
# Split into data matrix and response vector
X <- df %>% select(-arrest)
y <- df$arrest

# Initialise lr class
lr <- LogisticRegression$new(solver = "BFGS",
                             control = list(maxit=1000, reltol = 1e-4),
                             round_y_hat = TRUE)

# Find repeated cv error
lr_results <- kfold_cv(lr, X, y, metrics, k = 5, n_reps = 5,
                       parallel = TRUE, n_threads = 5)


as_tibble(lr_results)
# Export results for use in report
xtable(as_tibble(lr_results), digits = 4)
```

```{r echo=FALSE, eval = T}
X <- df %>% select(-arrest)
y <- df$arrest

lr_results <- readRDS("../analysis/lr_results.rds")

as_tibble(lr_results)
# Export results for use in report
xtable(as_tibble(lr_results), digits = 4)
```

### Support Vector Machine
As the support vector machine allows use of a kernel function, it should be able to capture non-linear relationships in the original feature space (given an appropriate kernel). Here, a radial basis function kernel is used. This implementation is a wrapper of the SVM package `e1071`. Unfortunately, support vector machines do not scale well to large data sets, so here we will only use 30000 rows.

We first need to find the optimal hyperparameters which we do by conducting a grid search. We use a smaller data set here as the models need to be fitted many times. 

```{r, echo=TRUE, eval=FALSE}
idxs <- sample(1:nrow(df), 20000)
X_tune <- X[idxs, ]
y_tune <- y[idxs]

svm_tuned <- tune(e1071::svm, y ~ ., data = cbind(X_tune, y_tune),
          ranges = list(cost = 2^(-2:5),gamma = 2^(-15:-4)),
          tunecontrol = tune.control(nrepeat = 3,
          sampling = "cross", cross = 5),
          kernel = "radial", type = "C-classification")

svm_tuned
```

```{r, echo = FALSE, eval = TRUE}
svm_tuned <- readRDS("../analysis/svm_tuned.rds")

svm_tuned
```

With these hyperparameters we then find the 5x repeated cross-validated error on a subset with 30,000 rows.

```{r, cache = TRUE, echo = T, eval = F}
idxs <- sample(1:nrow(df), 30000)
X <- X[idxs, ]
y <- y[idxs]
svm <- SupportVectorMachine$new(kernel = "radial")

svm_results <- kfold_cv(svm, X, y, metrics, 5, 5, parallel = FALSE,
                        n_threads = 5, gamma = 2^-5, cost = 2^4)




# Export results for use in report
as_tibble(svm_results)
xtable(as_tibble(svm_results), digits = 4)

```

```{r, eval = T, echo = F}
svm_results <- readRDS("../analysis/svm_results.rds")
as_tibble(svm_results)
xtable(as_tibble(svm_results), digits = 4)
```
### Random Forest
The random forest trains multiple decision trees using a subset of features to create each tree. Decision trees are an ordered set of rules defined on the feature values that attempt to split the data into classes. They have a tendency to overfit, the random forest solves this by growing multiple trees on different features within the dataset. In the classification case a prediction is made by predicting from each decision tree separately and finding the most common predicted class. They are a black-box method but can be useful for prediction. The R6 class used here uses the `randomForest` package internally. Due to the high computational cost involved in fitting random forest models we again use a randomly selected subset of the data (20000 rows) as we did for the SVM.

```{r, eval = F, echo = T}
X <- select(X, - "community_area")  # Can't handle feature with so many levels.

# Initialise random forest class
rf <- RandomForest$new()

rf_results <- kfold_cv(rf, X, y, metrics, 5, 5, parallel = TRUE, n_threads = 5)


# Export results for use in report
as_tibble(rf_results)
xtable(as_tibble(rf_results), digits = 4)

```

```{r, eval = T, echo = F}
idxs <- sample(1:nrow(df), 30000)
X <- X[idxs, ]
y <- y[idxs]

X <- select(X, - "community_area") 
rf_results <- readRDS("../analysis/rf_results.rds")
rf <- RandomForest$new()
as_tibble(rf_results)
xtable(as_tibble(rf_results), digits = 4)
```



## Comparison of models
The models all performed very similarly. Unsurprisingly, the support vector machine and the random forest classifier performed better, due to their ability to utilise non-linear relationships. It is worth noting that this difference is surprisingly small, although this may be due to the fact that logistic regression could be trained using more data. Alternatively, it could suggest that encoding the community areas as factors was sufficient to capture much of the spatial aspect of the crimes in logistic regression.

Logistic regression had higher sensitivity than the support vector machine, so could be a preferable option if false negatives are particularly detrimental, although it is worth noting that both support vector machines and logistic regression can be adjusted to reflect unbalanced costs associated with incorrect predictions.

Being an ensemble model, the random forest unsurprisingly performed the best based on all three metrics. It is likely that the random forest could be improved by training on a larger subset of the data and optimising the hyperparameters. The downside of this is that as a black-box method is does not provide any inference as to how the features are used within the model. We are at least able to look at how many times each feature was chosen as a splitting point in all of the trees providing some inference as to the importance of each of the features. 


```{r, eval = T, warning=FALSE}
# Fit model 
rf$fit(X, y)

# get importance of features from model
imp <- importance(rf$fitted_model)
imp <- tibble(feature = row.names(imp), mean_decrease_gini = as.vector(imp))

imp <- imp %>%
  arrange(desc(mean_decrease_gini)) %>%
  mutate(feature = factor(feature, levels = .$feature))

# Plot importance
imp_plot <- ggplot(aes(x = feature, y = mean_decrease_gini), data = imp) + geom_col(fill = "steelblue2") + ylab("Mean decrease in Gini")
imp_plot
ggsave("../plots/rf_imp.pdf", plot = imp_plot, width = 7.5, height = 5)
```

We see the most important feature was `fbi_code`, this is somewhat expected as it determines what type of crime occurred and this would intuitively have a large affect in whether someone was arrested. None of the models achieve an accuracy of more than 90%. The availability of more data, such as the demographic background of the suspect, would likely allow for more accurate predictions.
