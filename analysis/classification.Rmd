---
title: "Arrest Classification"
date: "18/01/2021"
output: pdf_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = paste0(rprojroot::find_rstudio_root_file(), "/chigcrim"))
```

# Imports
```{r message=FALSE}
library(tidyverse)
library(data.table)
library(lubridate)
library(caret)
library(e1071)
library(randomForest)

devtools::load_all("../chigcrim/")

set.seed(1)
```


# Introduction
Predicting if a suspect is arrested is a binary classification problem. To assess performance, three metrics will be used, the overall accuracy, the sensitivity and the specificity. These are defined as follows:

- Overall accuracy: The proportion of correct predictions.

- Specificity: Proportion of negative observations correctly predicted.

- Sensitivity: Proportion of positive observations correctly predicted.

Here, two classifiers are considered, logistic regression and a support vector machine classifier. Note that as logistic regression is a probabilistic classifier, the results are rounded to yield predictions that can be assessed with the metrics above to facilitate the comparison with the support vector machine results.

# Feature selection
Due to computational limitations, only the 2019 data will be considered. Below, the feature selection choices are outlined:

- Encode the date as the day in the year (1-365).

- Encode time of day as a float 0-24 (this looks OK from a plot of proportion arrested over time).

- Drop `year` as these are all 2019.

- We will drop the `id`s, as it contains unique values.

- We drop `case_number`, as it contains almost all unique values.

- Drop `primary_type`, `description` and `iucr` code. Keep `fbi_code` as an indicator of crime type.

- Assume `updated_on` is not informative (see EDA).

- `latitude` and `longitude` are dropped. `x_coordinates` and `y_coordinates` are kept. Note that these coordinates likely not particularly useful for linear classifiers, but should be useful for non-linear methods.

- Community areas will be kept, but other areas (`district`, `beat`, `ward` and `block`) are dropped.

- NAs will be dropped (see EDA).

- Particularly rare factors will be grouped into a variable other (see `?otherise`).


```{r}
df <- load_data(year = 2019, strings_as_factors = FALSE)

df$fbi_code <- otherise(df$fbi_code, 500)
df$location_description <- otherise(df$location_description, 1000)

remove_features <- c("id", "year", "case_number", "primary_type", "description",
                     "iucr", "updated_on", "latitude", "longitude",
                     "date", "district", "beat", "ward", "block")

df <- df %>%
  mutate(day = yday(df$date),
         time = hour(df$date) + minute(df$date)/60) %>%
  select(-all_of(remove_features)) %>%
  filter(complete.cases(df))

# Convert to factors
df <- df %>%
  mutate(location_description = as.factor(location_description),
         fbi_code = as.factor(fbi_code),
         community_area = as.factor(community_area))

head(df)
```

## Logistic Regression
We will first consider a logistic regression classifier. The factor variables are one-hot-encoded internally. Here, we use repeated k-fold cross validation. Specifically, 5-fold cross validation is repeated three times, and the metrics are averaged across the folds.
```{r time_it=TRUE}
X <- df %>% select(-arrest)
y <- df$arrest

lr <- LogisticRegression$new(solver = "BFGS",
                             control = list(maxit=1000, reltol = 1e-4),
                             round_y_hat = TRUE)

metrics <- list(accuracy = classification_accuracy,
                sensitivity = classification_sensitivity,
                specificity = classification_specificity)

#REMOVE THIS
X <- X[1:1000, ]
y <- y[1:1000]

lr_results <- kfold_cv(lr, X, y, metrics,
                       k = 5, n_reps = 3, parallel = TRUE, n_threads = 3)


as.tibble(lr_results)

```

## Support Vector Machine
As the support vector machine allows use of a kernel function, it should be able to capture non-linear relationships in space (given an appropriate kernel). Here, a radial basis function kernel is used. Unfortunately, support vector machines do not scale well to large data sets, so here we will only use 20000 rows for training.
```{r}
# Make train and test data (normally want train > test
# but SVM won't compute with large number of rows)
train_idxs <- sample(1:nrow(df), 20000)
train <- df[train_idxs, ]
test <- df[-train_idxs, ]

# Create svm using e1071 package
svm_ <- svm(arrest ~ ., data = train, type = "C-classification", kernel = "radial", cachesize = 120)

y_hat <- predict(svm_, newdata = test)
svm_results <- confusionMatrix(y_hat, as.factor(test$arrest))

```

## Random Forest
The random forest trains multiple decision trees using a subset of features to create each tree. Decision trees are an ordered set of rules defined on the feature values that attempt to split the data into classes. They have a tendency to overfit, the random forest solves this by growing multiple trees on different features within the datset. In the classification case a prediction is made by predicting from each decision tree seperately and finding the most common predicted class. They are a black-box method but can be useful for prediction. We train our random forest using the `randomForest` package. Due to the high computational cost involed in fitting random forest models we train it on a subset of the data taking 20000 rows as we did for the SVM.

```{r}
# randomForest can't take categorical variables with more than 53 levels, as spatial information should be captured
# by long/lat we remove this from the data set
rf_train <- select(train, - "community_area") 
rf <-randomForest(select(rf_train, -"arrest"), y = as.factor(rf_train$arrest))
y_hat <- predict(rf, newdata = test)
rf_results <- confusionMatrix(y_hat, as.factor(test$arrest))
```

## Comparison of models
The results for the models are shown below:

**Logistic regression**:

- Overall accuracy: `r lr_results$overall["Accuracy"]`

- Sensitivity: `r lr_results$byClass["Sensitivity"]`

- Specificity: `r lr_results$byClass["Specificity"]`

**Support Vector Machine**

- Overall accuracy: `r svm_results$overall["Accuracy"]`

- Sensitivity: `r svm_results$byClass["Sensitivity"]`

- Specificity: `r svm_results$byClass["Specificity"]`

**Random Forest**

- Overall accuracy: `r rf_results$overall["Accuracy"]`

- Sensitivity: `r rf_results$byClass["Sensitivity"]`

- Specificity: `r rf_results$byClass["Specificity"]`

Surprisingly, despite the ability of support vector machines to utilise non-linear relationships (in the original feature space), it did not perform better than logistic regression (and was much more computationally costly to train). The support vector machine had higher sensitivity. This reflects the fact that the logistic regression classifier gave more balanced predictions between the classes, inevitably leading to more false negatives.

Perhaps these results suggest that much of the spatial relationship in the data could be captured using the `community_area` and `location_description` factor variables, rather than the specific x and y coordinates of the crime. Hence, the logistic regression classifier could perform well, despite the data being spatial, and likely infringing on the independent and identically distributed assumption of logistic regression. Alternatively, perhaps the performance of the support vector machine was hindered by the fact it was only trained on a small proportion of the data set (due to computational limitations), compared to logistic regression.

On the other hand the random forest managed to out perform both the other models without using `community area` as a feature. It is likely that the random forest could be improved by training on a larger subset of the data and optimising the hyperparameters. The downside of this is that as a black-box method is does not provide any inference as to how the features are used within the model. We are at least able to look at how many times each feature was chosen as a splitting point in all of the trees providing some inference as to the importance of each of the features. 

```{r}
importance(rf)
```

We see the most important feature was `fbi_code`, this is somewhat expected as it determines what type of crime occcured and this would intuitively have a large affect in whether someone was arrested. None of the models achieve an accuracy of more than 90%. This could be due to the absence of key demographic data which may provide more information as to whether someone will be arrested.
