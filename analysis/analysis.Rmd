---
title: "Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Imports
```{r}
library(tidyverse)
library(data.table)
library(lubridate)
library(caret)

devtools::load_all("../chigcrim/")
```

## Load data
```{r}
df <- load_data(year = 2019, strings_as_factors = FALSE)
```


# Predicting if a suspect is arrested
Predicting if a suspect is arrested is a binary classification problem. Here logistic regression will be used to perform binary classification. This has the assumption that the data is independent and identically distributed, which likely is not the case.

## Feature selection
Choices:
- Encode the date as the day in the year (1-365)

- Encode time of day as a float 0-24 (this looks OK from a plot of proportion arrested over time).

- Drop `year` as these are all 2019

- We will drop the `id`s, as it contains unique values

- We drop `case_number`, as it contains almost all unique values

- Drop `primary_type`, `description` and `iucr` code. Keep `fbi_code` as an indicator of crime type.

- Assume `updated_on` is not informative (see EDA)

- X and Y coordinates, latitude and longitude are dropped (a linear relationship does not seem like it would be very useful)

- Community areas will be kept, but other areas (`district`, `beat`, `ward` and `block`) are dropped.

- NAs will be dropped

```{r}
df$fbi_code <- otherise(df$fbi_code, 500)
df$location_description <- otherise(df$location_description, 1000)

remove_features <- c("id", "year", "case_number", "primary_type", "description",
                     "iucr", "updated_on", "latitude", "longitude",
                     "x_coordinate", "y_coordinate", "date",
                     "district", "beat", "ward", "block")

df <- df %>%
  mutate(day = yday(df$date),
         time = hour(df$date) + minute(df$date)/60) %>%
  select(-all_of(remove_features)) %>%
  filter(complete.cases(df))

# Convert to factors
df <- df %>%
  mutate(location_description = as.factor(location_description),
         fbi_code = as.factor(fbi_code),
         community_area = as.factor(community_area))
```


```{r}
df
```


Run logistic-regression with cross-validation. Due to the size of the data set, running several folds would take a while so only a single fold is used.
```{r time_it=TRUE}
X <- df %>% select(-arrest)
y <- df$arrest

n_test <- round(0.2*nrow(X))
test_idxs <- sample(1:nrow(X), n_test)

X_train <- X[-test_idxs, ]
X_test <- X[test_idxs, ]
y_train <- y[-test_idxs]
y_test <- y[test_idxs]

lr <- LogisticRegression$new(solver = "BFGS")
lr$fit(X_train, y_train, control = list(maxit=1000, reltol = 1e-4))
y_hat <- lr$predict(X_test)
```

We can analyse the results of the classification using a confusion matrix:
```{r}
y_hat <- round(y_hat)
y_hat <- as.logical(y_hat)

confusionMatrix(as.factor(y_hat), as.factor(y_test))
```




# Interpolating number of crimes using kernel ridge regression
Another question is whether the number of crimes on particular days can be estimated. A simple way to do this is to use a single feature, the week, and to interpolate between the points using kernel ridge regression. The kernel ridge regression implementation includes a linear kernel (equivalent to ridge regression), a polynomial kernel, and a radial basis function kernel. As the relationship complex and non-linear, only the radial basis function feature transform will be considered here.

### Sort out data set
```{r}
df <- fread("../data/crimes-2001-present.csv", select = c("Date"))

df <- df %>%
  mutate(date = as.Date(mdy_hms(Date))) %>%
  select(-Date) %>%
  # filter(date >= "2015-01-01") %>% # Last 5 years
  mutate(week = week(date),
         year = year(date)) %>%
  filter(week != 53)  # 53rd "week" does not contain 7 days

df <- df %>%
  group_by(week, year) %>%
  mutate(week_start = min(date))

df <- df %>%
  group_by(week, week_start, year) %>%
  tally(name = "n_crimes")

df <- df[order(df$year, df$week),]
df$cumulative_week <- 1:nrow(df)
```




### Cross-validation
Below I will use 5-fold cross validation to choose the bandwidth hyperparameter for the radial basis function kernel.
```{r message = FALSE}
X <- as.matrix(df$cumulative_week)
X <- scale(X)
y <- df$n_crimes

bandwidth <- c(0.1, 1, 5, 10, 100)

mse_vec <- c()
set.seed(2)
for (i in 1:length(bandwidth)){
  kr = KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  mse <- cv_R6_k_fold(kr, X, y, squared_error_loss, 5)
  mse_vec <- c(mse_vec, mse)
  
  # Save best model
  if (mse == min(mse_vec)){
    kr_best <- kr$clone(deep=TRUE)
  }
}

rbf_results <- data.frame(bandwidth, mse = mse_vec)

rmse <- sqrt(rbf_results$mse[rbf_results$bandwidth == kr_best$A])
print(sprintf("Optimal bandwidth found: %s", kr_best$A))
print(sprintf("Which had root mean square error: %s", rmse))

ggplot(rbf_results) +
  geom_line(aes(x = bandwidth, y = mse)) +
  scale_x_log10() +
  scale_y_log10()
```

A bandwidth of `r kr_best$A` seems reasonable, gave the lowest cross-validation error. As this is single dimensional we can plot the prediction function:

```{r}
y_hat = kr_best$predict(X)

# Plot output
df$y_hat <- y_hat
df
p <- ggplot(df) + 
  geom_point(aes(x = week_start, y = n_crimes)) +
  geom_line(aes(x = week_start, y = y_hat), colour = "steelblue2", size = 1) +
  labs(x = "Year", y = "Number of crimes")
p
```

Another option that has not been considered here is using a trigonometric transform of the data (with a period of a year), to try and capture the cycles with a simpler model. It is worth noting that whilst useful for interpolation, kernel ridge regression with a radial basis function kernel would have limited utility for forecasting crimes for into the future. We can see why this is if we try to extrapolate using these this model.

```{r}
start_week_date <- max(df$week_start) + 7
start_week_int <- max(df$cumulative_week) + 1
  
extra_dates <- seq(ymd(start_week_date),ymd(start_week_date + 365*4),
                   by = '1 week')

extra_X <- start_week_int:(start_week_int+length(extra_dates)-1)
extra_X <- (extra_X - attributes(X)$`scaled:center`) /
  attributes(X)$`scaled:scale`

extra_y_hat <- kr_best$predict(as.matrix(extra_X))


extra_df <- tibble(extra_dates, extra_y_hat)
p +
  geom_line(data = extra_df, aes(extra_dates, extra_y_hat), colour = "steelblue2")
```

The cycles are captured solely due to being able to interpolate locally between points. Hence, when extrapolating with a radial basis function kernel, no cycles are predicted, so the out-of-sample accuracy would drop substantially. The cross-validation error found is only applicable if the new data followed the same distribution, and that is not the case when considering future data.

### Predict number of crimes in a particular month for a particular location
The previous example used a single dimensional feature vector. We can also apply it to more complicated problems. Here we will consider the problem of predicting the number of crimes in a given month, for different community areas. In order to make use of the spatial aspect of the data, the position of a community area is approximated using the mean x and y coordinates for crimes within that community.

Here, for computational reasons, only the last 5 years will be considered.
```{r}
# Can use below but slow so better to download data set from website
# df <- load_data(strings_as_factors = FALSE)

df <- fread("../data/crimes-2001-present.csv",
             select = c("Date", "X Coordinate", "Community Area",
                        "Y Coordinate"))

colnames(df) <- str_replace(tolower(colnames(df)), " ", "_")
```


```{r}
df <- df %>%
  mutate(date = as.Date(mdy_hms(date)),
         year = year(date),
         month = month(date)) %>%
  filter(date >= "2015-01-01", year != 2021) %>%  # Not enough data on 2021 yet
  drop_na()

df <- df %>%
  group_by(community_area) %>%
  mutate(x_coordinate = mean(x_coordinate),
         y_coordinate = mean(y_coordinate)) %>%
  group_by(year, month) %>%
  mutate(date = min(date)) %>%
  group_by_all() %>%
  summarise(n_crimes = n())

```

```{r}
X <- df %>%
  ungroup() %>%
  select(-community_area, -date, -n_crimes) %>%
  as.matrix()

X <- scale(X)
y <- df$n_crimes


bandwidth <- c(0.1, 1, 5, 10, 100, 1000)

mse_vec <- c()
set.seed(3)
for (i in 1:length(bandwidth)){
  kr = KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  mse <- cv_R6_k_fold(kr, X, y, squared_error_loss, 5)
  mse_vec <- c(mse_vec, mse)
  
  # Save best model
  if (mse == min(mse_vec)){
    kr_best <- kr$clone(deep=TRUE)
  }
  print(i)
}

rbf_results <- data.frame(bandwidth, mse = mse_vec)

rmse <- sqrt(rbf_results$mse[rbf_results$bandwidth == kr_best$A])
print(sprintf("Optimal bandwidth found: %s", kr_best$A))
print(sprintf("Which had root mean square error: %s", rmse))

ggplot(rbf_results) +
  geom_line(aes(x = bandwidth, y = mse)) +
  scale_x_log10() +
  scale_y_log10()

```

Although being higher dimensional so difficult to plot, we can plot the predicted values against the observed values on a held out data set:
```{r}
test_idxs <- sample(1:nrow(X), round(0.2*nrow(X)))

X_train <- X[-test_idxs, ]
y_train <- y[-test_idxs]
X_test <- X[test_idxs, ]
y_test <- y[test_idxs]

# Refit best model
kr_best$fit(X_train, y_train)
y_hat <- kr_best$predict(X_test)

qplot(y_test, y_hat, geom = "point") +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  labs(x = "Actual number of crimes", y = "Predicted number of crimes")
```

We can see a good association between the actual number of crimes in a given month and community area, and the predicted number of crimes on this held-out data. Once again, this method would primarily be useful for interpolation, rather than extrapolation, for similar reasons as have been mentioned above.

