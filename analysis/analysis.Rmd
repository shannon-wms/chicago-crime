---
title: "Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Imports
```{r}
library(tidyverse)
library(data.table)
library(lubridate)
library(caret)

devtools::load_all("../chigcrim/")
```

## Load data
```{r}
df <- load_data(year = 2019, strings_as_factors = FALSE)
```


# Predicting if a suspect is arrested
Predicting if a suspect is arrested is a binary classification problem. Here logistic regression will be used to perform binary classification. This has the assumption that the data is independent and identically distributed, which likely is not the case.

## Feature selection
Choices:
- Encode the date as the day in the year (1-365)

- Encode time of day as a float 0-24 (this looks OK from a plot of proportion arrested over time).

- Drop `year` as these are all 2019

- We will drop the `id`s, as it contains unique values

- We drop `case_number`, as it contains almost all unique values

- Drop `primary_type`, `description` and `iucr` code. Keep `fbi_code` as an indicator of crime type.

- Assume `updated_on` is not informative (see EDA)

- X and Y coordinates, latitude and longitude are dropped (a linear relationship does not seem like it would be very useful)

- Community areas will be kept, but other areas (`district`, `beat`, `ward` and `block`) are dropped.

- NAs will be dropped

```{r}
df$fbi_code <- otherise(df$fbi_code, 500)
df$location_description <- otherise(df$location_description, 1000)

remove_features <- c("id", "year", "case_number", "primary_type", "description",
                     "iucr", "updated_on", "latitude", "longitude",
                     "x_coordinate", "y_coordinate", "date",
                     "district", "beat", "ward", "block")

df <- df %>%
  mutate(day = yday(df$date),
         time = hour(df$date) + minute(df$date)/60) %>%
  select(-all_of(remove_features)) %>%
  filter(complete.cases(df))

# Convert to factors
df <- df %>%
  mutate(location_description = as.factor(location_description),
         fbi_code = as.factor(fbi_code),
         community_area = as.factor(community_area))
```

Run logistic-regression with cross-validation. Due to the size of the data set, running several folds would take a while so only a single fold is used.
```{r time_it=TRUE}
X <- df %>% select(-arrest)
y <- df$arrest

n_test <- round(0.2*nrow(X))
test_idxs <- sample(1:nrow(X), n_test)

X_train <- X[-test_idxs, ]
X_test <- X[test_idxs, ]
y_train <- y[-test_idxs]
y_test <- y[test_idxs]

lr <- LogisticRegression$new(solver = "BFGS")
lr$fit(X_train, y_train, control = list(maxit=1000, reltol = 1e-4))
y_hat <- lr$predict(X_test)
```

We can analyse the results of the classification using a confusion matrix:
```{r}
y_hat <- round(y_hat)
y_hat <- as.logical(y_hat)

confusionMatrix(as.factor(y_hat), as.factor(y_test))
```


# Interpolating number of crimes using kernel ridge regression
Another question is whether the number of crimes on particular days can be estimated. A simple way to do this is to use a single feature, the week, and to interpolate between the points using kernel ridge regression. The kernel ridge regression implementation includes a linear kernel (equivalent to ridge regression), a polynomial kernel, and a radial basis function kernel. As the relationship complex and non-linear, only the radial basis function feature transform will be considered here.

### Sort out data set
```{r}
df <- fread("../data/crimes-2001-present.csv", select = c("Date"))

df <- df %>%
  mutate(date = as.Date(mdy_hms(Date))) %>%
  select(-Date) %>%
  # filter(date >= "2015-01-01") %>% # Last 5 years
  mutate(week = week(date),
         year = year(date)) %>%
  filter(week != 53)  # 53rd "week" does not contain 7 days

df <- df %>%
  group_by(week, year) %>%
  mutate(week_start = min(date))

df <- df %>%
  group_by(week, week_start, year) %>%
  tally(name = "n_crimes")

df <- df[order(df$year, df$week),]
df$cumulative_week <- 1:nrow(df)
```




### Cross-validation
Below I will use 5-fold cross validation to choose the bandwidth hyperparameter for the radial basis function kernel.
```{r message = FALSE}
X <- as.matrix(df$cumulative_week)
X <- scale(X)
y <- df$n_crimes

bandwidth <- c(0.1, 1, 5, 10, 100)

mse_vec <- c()
set.seed(2)
for (i in 1:length(bandwidth)){
  kr = KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  mse <- cv_R6_k_fold(kr, X, y, squared_error_loss, 5)
  mse_vec <- c(mse_vec, mse)
  
  # Save best model
  if (mse == min(mse_vec)){
    kr_best <- kr$clone(deep=TRUE)
  }
}

rbf_results <- data.frame(bandwidth, mse = mse_vec)

rmse <- sqrt(rbf_results$mse[rbf_results$bandwidth == kr_best$A])
print(sprintf("Optimal bandwidth found: %s", kr_best$A))
print(sprintf("Which had root mean square error: %s", rmse))

ggplot(rbf_results) +
  geom_line(aes(x = bandwidth, y = mse)) +
  scale_x_log10() +
  scale_y_log10()
```

A bandwidth of `r kr_best$A` seems reasonable, gave the lowest cross-validation error. As this is single dimensional we can plot the prediction function:

```{r}
y_hat = kr_best$predict(X)

# Plot output
df$y_hat <- y_hat
df
p <- ggplot(df) + 
  geom_point(aes(x = week_start, y = n_crimes)) +
  geom_line(aes(x = week_start, y = y_hat), colour = "steelblue2", size = 1) +
  labs(x = "Year", y = "Number of crimes")
p
```
Another option that has not been considered here is using a trigonometric transform of the data (with a period of a year), to try and capture the cycles with a simpler model.

It is worth noting that whilst useful for interpolation, kernel ridge regression with a radial basis function kernel would have limited utility for forecasting crimes for into the future. We can see why this is if we try to extrapolate using these this model:

```{r}
start_week_date <- max(df$week_start) + 7
start_week_int <- max(df$cumulative_week) + 1
  
extra_dates <- seq(ymd(start_week_date),ymd(start_week_date + 365*4),
                   by = '1 week')

extra_X <- start_week_int:(start_week_int+length(extra_dates)-1)
extra_X <- (extra_X - attributes(X)$`scaled:center`) /
  attributes(X)$`scaled:scale`

extra_y_hat <- kr_best$predict(as.matrix(extra_X))


extra_df <- tibble(extra_dates, extra_y_hat)
p +
  geom_line(data = extra_df, aes(extra_dates, extra_y_hat), colour = "steelblue2")
```

The cycles are captured solely due to being able to interpolate locally between points. Hence, when extrapolating with a radial basis function kernel, no cycles are predicted, so the out-of-sample accuracy would drop substantially.

### Predict number of crimes in a particular month for a particular location
The previous example used a single dimensional feature vector. We can also apply it to more complicated problems. Here we will consider the problem of predicting the number of crimes in a given month, for different community areas. In order to make use of the spatial aspect of the data, the position of a community area is approximated using the mean x and y coordinates for crimes within that community.


```{r}
# Can use below but slow so better to download data set from website
# df <- load_data(strings_as_factors = FALSE)

df <- fread("../data/crimes-2001-present.csv",
             select = c("Date", "X Coordinate", "Community Area",
                        "Y Coordinate"))

colnames(df) <- str_replace(tolower(colnames(df)), " ", "_")
```



```{r}
df <- df %>%
  mutate(date = as.Date(mdy_hms(date))) %>%
  mutate(year = year(date),
         month = month(date)) %>%
  drop_na()
df
df %>%
  group_by(community_area) %>%
  mutate(x_coordinate = mean(x_coordinate),
         y_coordinate = mean(y_coordinate)) %>%
  ungroup() %>%
  group_by(c(community_area, year, month)) %>%   ### LEFT OFF HERE
  summarise(tally())





```


```{r}
df
```


```{r}
df
df %>%
  mutate()

Date(mdy_hms(df$date[1])

df$date <- Date(mdy_hms(df$date))
df$date
```


```{r}
df <- load_data(year = 2019, strings_as_factors = FALSE)
```


```{r}
df <- df %>%
  drop_na() %>%
  mutate(month = month(date)) %>%
  select(c(x_coordinate, community_area, y_coordinate, month)) %>%
  group_by(community_area) %>%
  mutate(x_coordinate = mean(x_coordinate),
         y_coordinate = mean(y_coordinate)) %>%  
  group_by(community_area, month, x_coordinate, y_coordinate) %>%
  summarise(n_crimes = n(), .groups = "drop")
```




```{r}
X <- df %>% select(-n_crimes) %>% as.matrix()
y <- df$n_crimes

X <- scale(X)
# X <- X[1:500, ]
# y <- y[1:500]

library(tictoc)
tic()
bandwidth <- c(0.1, 1, 10, 100)
errors <- c()
for (i in 1:length(bandwidth)){
  kr <- KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  err <- cv_R6(kr, X, y, squared_error_loss, 0.2)
  errors <- c(errors, err)
  print(i)
}
toc()

ggplot() +
  geom_line(aes(x=bandwidth, y=errors)) +
  scale_x_log10()

```


```{r}
X <- df %>% select(-n_crimes) %>% as.matrix()
y <- df$n_crimes


tic()
kr <- KernelRidge$new("rbf", lambda = 0.001, 1)
cv_R6(kr, X, y, squared_error_loss, 0.2)
toc()
```


Technically calculating the mean coordinates like this (which will include values from the test set), could cause data leakage, but it likely does not matter (similar to scaling using the whole data set).




#########
Could use extra data of population size?

# Include size of community areas ???
```{r}
df <- df %>%
  mutate(date = as.Date(mdy_hms(date))) %>%
  filter(date >= "2019-01-01") %>% # Last 5 years
  mutate(year = year(date),
         month = month(date),
         description = paste(primary_type, description)) %>%
  select(-primary_type) %>%
  drop_na()
df
df <- df %>%
  group_by(community_area) %>%
  mutate(x_center = mean(x_coordinate), y_center = mean(y_coordinate)) #%>%
  summarise(n_crimes = n(), .groups = c(month, year, community_area, x_center, y_center))




```


