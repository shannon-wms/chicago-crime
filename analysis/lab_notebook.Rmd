---
title: "Compass TB1 Group Project: Lab Notebook"
author: "Euan Enticott, Daniel Ward, Shannon Williams"
date: "25/01/2021"
output: 
  pdf_document:
    number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = paste0(rprojroot::find_rstudio_root_file()))
```

# Introduction

In this lab notebook we present the analyses undertaken as part of the Compass TB1 group project. We focus on a large, open source dataset provided by the City of Chicago consisting of reported incidents of crime from 2001 to present day, as reported by the Chicago Police Department. The dataset can be obtained from the website \url{https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-Present/ijzp-q8t2}. 

The notebook will be broken up into three sections:

1. Workflow and Computational Techniques: A summary of the approach used in the development of the **chigcrim** package.

2. Classification: Predicting whether a reported crime led to the arrest of a suspect. We consider three different discriminative classifiers: logistic regression, support vector machines, and random forests.

3. Prediction: Predicting the number of reported crimes which occur in a given region over a specified time period (of a given type, if desired), through interpolation. We consider kernel ridge regression and generalised additive models (GAMs).

# Workflow and Computational Techniques

In the development of the package **chigcrim**, we utilised a number of computational techniques which are documented below. The package can be accessed on GitHub: \url{https://github.com/shannon-wms/chicago-crime}.

## Object-Oriented Programming

We decided to use an object oriented approach to constructing models. In particular, we made use of the `R6Class` from the **R6** package. `R6` objects are mutable and use an object oriented approach similar to most programming languages (unlike the more basic `S3` and `S4` classes). `R6` classes are very similar to reference classes, although generally `R6` classes are preferred. This is for a variety of reasons, including the fact that `R6` classes are much faster, and they handle fields in a neater way by putting them in a separate environment. More information on this can be found [here](https://adv-r.hadley.nz/r6.html). Notably, `R6` classes support both public (available to the user) and private (not visible to the user) members.

Each of the models considered can be initialised with a call to `model$new(...)`, and then fitted using `model$fit(X, y)` where `X` and `y` are our training data matrix and response vector, respectively. Predictions can then be made with `model$predict(X)`. The datasets, fitted model and predictions, along with other objects unique to the model, are then stored as fields accessed through `model$field`.

It is worth noting that this use of `fit` and `predict` methods in object-oriented machine learning packages is used elsewhere, particularly in the **Python** package **scikit-learn**. The key advantage of this approach is that it abstracts away the underlying complexities, and creates a consistent structure that makes it easier to compare different models. 

## Parallel Programming

Parallel programming was used to speed up slow code, utilising the package **doParallel** which calls the packages **parallel** and **foreach**. In particular, we implemented a parallel cross-validation function `kfold_cv`, which facilitated more accurate and faster assessment of model performance.

## Documentation

In order to document functions in the package, we made use of **roxygen2**. This package automatically creates the `.Rd` documentation files for functions, classes and data, from comments added above the function definitions. In addition to being an efficient way to generate documentation, this allows the code and documentation to coexist, making it easier to remember to update the documentation.

## API Querying

The Chicago Crime dataset is updated on a daily basis and, at the time of writing, contains over 7 million rows of data. For each reported incident, a total of 22 features are recorded (for a full list of features and their descriptions see the City of Chicago website). We elected not to download the dataset in full and include this in the package as it would consume a significant amount of storage, and quickly become out of date. 

Instead, we utilised the **RSocrata** package which allows easy interaction with the online open data portal. The wrapper `load_data` allows the end user to directly download the dataset into the R environment, via the function `read.socrata`. `load_data` provides functionality for filtering by year, limiting the number of data points received, and omitting missing/`NA` values.

## Testing and Continuous Integration

For tests, we used the **testthat** package. This simply creates a new directory `./tests/testthat/` in which the user can define tests for the package functions. Although the tests can be run alone, generally, we made use of the package **rcmdcheck**. This contains the function `rcmdcheck`, which not only runs the tests, but also carries out a more sophisticated check. Some examples of what is checked are listed below:

- Checks the package installs correctly.

- Checks for missing documentation.

- Automatically runs examples in documentation, to check they run successfully.

- Checks for undefined variables.

- Checks for missing dependencies.

In order to ensure that tests and checks were run on a regular basis, continuous integration was set up using Github Actions. This runs `rcmdcheck()` for each pull request and push to the main branch, thus limiting the possibility of unintentionally introducing any breaking changes.

\pagebreak

# Classification

In this section we consider the binary classification task of predicting whether a suspect is arrested, given data on the reported crime. Three models will be investigated: logistic regression, support vector machines and random forest classifiers.

## Assessing Performance

To assess performance, three metrics will be used:

- Overall accuracy: The proportion of correct predictions.

- Sensitivity: The proportion of positive observations correctly predicted.

- Specificity: The proportion of negative observations correctly predicted.

These metrics can be computed using the functions `classification_accuracy`, `classification_sensitivity`, and `classification_specifity`, respectively, from the **chigcrim** package.

Note that logistic regression is a probabilistic classifier whereas support vector machines and random forests are non-probabilistic. As such, the output from logistic regression is a probability in $[0,1]$ while the outputs from the other models are class predictions in $\{0,1\}$. In our analysis, the probabilities given by logistic regression are rounded to yield predictions that can be assessed using the metrics above. This facilitates the comparison of these results with the other two classifiers.

```{r, message = FALSE, warning = FALSE}
# Load required packages
library(chigcrim)

# Use seed to ensure reproducibility
set.seed(1)
```

```{r}
# List of metrics used to evaluate classification models
metrics <- list(accuracy = classification_accuracy,
                sensitivity = classification_sensitivity,
                specificity = classification_specificity)
```

For each classifier, we will use repeated $k$-fold cross validation to obtain mean values of the metrics. Specifically, 5-fold cross validation is repeated five times (run in parallel using the function `kfold_cv`), and the metrics are averaged across the folds and repeats.

\pagebreak

## Feature Selection

Due to the time and computational limitations involved in downloading, storing, and training models on the entire dataset, we shall train the models only on a subset of the data from the year 2019.

```{r, eval = FALSE}
# Load 2019 data to build classification models
df <- load_data(year = 2019, strings_as_factors = FALSE)
```

```{r, include = FALSE}
df <- readRDS("temp_data/data19.rds")
```

```{r}
print(as.data.frame(head(df)))
```

Below we outline our feature selection and data pre-processing choices. These decisions involved incorporating information from conducting exploratory data analysis (EDA), as well as considering what is computationally feasible.

- Encode the date as the day of the year, `yday` (an integer between 1-365, or 1-366 for leap years).

- Encode time of day as a floating point in 0-24 (`time`), as this showed promising results in our EDA.

- Drop `year` as we are only using data from 2019.

- Drop `id` as this is the unique key for each data point.

- We drop `case_number`, as the values are almost all unique and are not informative.

- Drop `primary_type`, `description` and `iucr` code. Keep `fbi_code` as an indicator of crime type.

- Drop `updated_on` as this only refers to when the data was last updated and is uninformative.

- Drop `latitude` and `longitude`, and keep `x_coordinate` and `y_coordinate`. Note that these coordinates are likely not particularly useful for linear classifiers, but should be useful for non-linear methods.

- Keep `community_area`, but other areas (`district`, `beat`, `ward` and `block`) are dropped.

- Omit all `NA` values.

- Particularly rare factors will be grouped into a variable other (see `?otherise`).

```{r}
# Convert rarely used fbi categories to "OTHER"
df$fbi_code <- otherise(df$fbi_code, 500)
df$location_description <- otherise(df$location_description, 1000)

# List of features for removal
remove_features <- c("id", "year", "case_number", "primary_type", "description",
                     "iucr", "updated_on", "latitude", "longitude",
                     "date", "district", "beat", "ward", "block")

# Add more time features, remove missing values and convert columns to factor
df %<>% mutate(day = yday(df$date), 
               time = hour(df$date) + minute(df$date) / 60) %>%
  select(-all_of(remove_features)) %>%
  filter(complete.cases(df)) %>%
  mutate(location_description = as.factor(location_description),
         fbi_code = as.factor(fbi_code),
         community_area = as.factor(community_area))

print(as.data.frame(head(df)))
```

\pagebreak

## Logistic Regression

We will first consider a logistic regression classifier, which is a linear classifier for binary data. We use our `R6` class `LogisticRegression` which has methods `fit` and `predict`. `fit` calls the function `optim()` internally to minimize the cross-entropy/log-loss function using gradient-based optimisation methods (either BFGS, L-BFGS-B or CG); in our analysis we use BFGS as we don't run into memory issues.

```{r, cache = TRUE}
# Split into data matrix and response vector
X <- df %>% select(-arrest)
y <- df$arrest
```

```{r, eval = FALSE}
# Initialise new LogisticRegression object
lr <- LogisticRegression$new(solver = "BFGS",
                             control = list(maxit = 1000, reltol = 1e-4),
                             rounding = TRUE)

# Find repeated cross-validation error
lr_results <- kfold_cv(lr, X, y, metrics, k = 5, n_reps = 5,
                       parallel = TRUE, n_threads = 5)

```

```{r, include = FALSE}
lr_results <- readRDS("temp_data/lr_results.rds")
```

```{r, cache = TRUE, results = "asis"}
# Export results for use in report
tab <- xtable::xtable(as_tibble(lr_results), digits = 4)
print(tab, type = "latex", comment = FALSE)
```

\pagebreak

## Support Vector Machines

As the support vector machine (SVM) allows use of a kernel function, it should be able to capture non-linear relationships in the original feature space (given an appropriate kernel). As the relationship between the features and the class is unknown, a radial basis function kernel is used. The `R6` class `SupportVectorMachine`, again with `fit` and `predict` methods, is a wrapper of the function `svm` from the dedicated SVM package **e1071**. Unfortunately, support vector machines do not scale well to large data sets, so here we will only use 30,000 rows.

```{r, cache = TRUE}
# Subset the data randomly
index <- sample(1:nrow(df), 30000)
X_subset <- X[index, ]
y_subset <- y[index]
```

In order to obtain optimal values for `cost` and `gamma` to be provided to `svm` for training the SVM, we utilise the built-in generic function `tune` from **e1071** which performs a grid search over given parameter ranges.

```{r, eval = FALSE}
svm_tuned <- tune(e1071::svm, y_subset ~ ., data = cbind(X_subset, y_subset), 
                  ranges = list(cost = 2^(-2:5), gamma = 2^(-15:-4)),
                  tunecontrol = tune.control(nrepeat = 3, 
                                             sampling = "cross", 
                                             cross = 5),
                  kernel = "radial", type = "C-classification")
```

```{r, include = FALSE}
svm_tuned <- readRDS("temp_data/svm_tuned.rds")
svm_results <- readRDS("temp_data/svm_results.rds")
```

```{r}
svm_tuned
```

We obtain optimal values of `2^4` for `cost` and `2^-5` for `gamma`, which we use to train our SVM.

```{r, cache = TRUE}
# Initialise new SupportVectorMachine object
svm <- SupportVectorMachine$new(kernel = "radial")
```

```{r, eval = FALSE}
# Run repeated 5-fold cross-validation to evaluate performance
svm_results <- kfold_cv(svm, X_subset, y_subset, metrics, k = 5, 
                        n_reps = 5, parallel = FALSE,
                        n_threads = 5, gamma = 2^-5, cost = 2^4)
```

```{r, cache = TRUE, results = "asis"}
# Export results for use in report
tab <- xtable::xtable(as_tibble(svm_results), digits = 4)
print(tab, type = "latex", comment = FALSE)
```

\pagebreak

## Random Forests

The random forest trains multiple decision trees using a subset of features to create each tree. Decision trees are an ordered set of rules defined on the feature values that attempt to split the data into classes. As decision trees have a tendency to overfit, the random forest attempts to overcome this by growing multiple trees on different features within the dataset. In the classification case, a prediction is made by predicting from each decision tree separately and finding the most common predicted class. This is a black-box method but has the potential for good predictive performance.

Our `R6` class `RandomForest` is a wrapper for the function `randomForest` from the **randomForest** package. Due to the high computational cost involved in fitting random forest models we again use a smaller subset of the data. Here we use the same dataset used to fit the SVM, with the factor `community_area` removed as the algorithm for fitting the model cannot handle such a large number of factors.

```{r, cache = TRUE}
X_subset <- select(X_subset, - "community_area")

# Initialise new RandomForest object
rf <- RandomForest$new()
```

```{r, include = FALSE}
rf_results <- readRDS("temp_data/rf_results.rds")
```

```{r, eval = FALSE}
# Run repeated 5-fold cross-validation to evaluate performance
rf_results <- kfold_cv(rf, X_subset, y_subset, metrics, k = 5, 
                       n_reps = 5, parallel = TRUE, n_threads = 5)
```

```{r, cache = TRUE, results = "asis"}
# Export results for use in report
tab <- xtable::xtable(as_tibble(rf_results), digits = 4)
print(tab, type = "latex", comment = FALSE)
```

## Model Comparisons

The models all performed quite similarly. Unsurprisingly, the support vector machine and the random forest classifier performed best, due to their ability to capture non-linear relationships. It is worth noting that this difference is surprisingly small, although this could be related to the fact that the logistic regression model was trained on a larger dataset. Alternatively, the SVM results could suggest that encoding the community areas as factors was sufficient to capture much of the spatial aspect of the crimes in logistic regression.

Logistic regression had higher sensitivity than the support vector machine, and thus could be viewed as a preferable option if false negatives are particularly detrimental. It is worth noting however that both support vector machines and logistic regression can be adjusted to reflect unbalanced costs associated with incorrect predictions.

Being an ensemble model, the random forest unsurprisingly performed the best based on all three metrics. It is likely that the random forest could be improved by training on a larger subset of the data and optimising the hyperparameters. The downside of using a random forest is that, as a black-box method, it does not provide any inference as to how the features are used within the model. We are at least able to look at how many times each feature was chosen as a splitting point in all of the trees, providing us with some information as to the importance of each of the features. 

```{r, warning = FALSE, cache = TRUE}
# Fit model with optimal hyper parameters
rf$fit(X_subset, y_subset)

# Get importance of features from model
imp <- randomForest::importance(rf$fitted_model)
imp <- tibble(feature = row.names(imp), mean_decrease_gini = as.vector(imp))

imp <- imp %>%
  arrange(desc(mean_decrease_gini)) %>%
  mutate(feature = factor(feature, levels = .$feature))

# Plot importance
imp_plot <- ggplot(aes(x = feature, y = mean_decrease_gini), data = imp) + 
  geom_col(fill = "steelblue2") + ylab("Mean decrease in Gini") + 
  scale_x_discrete(labels = c("FBI Code", "Y-Coordinate", "Time", "X-Coordinate", 
                              "Day", "Location Description", "Domestic")) + 
  theme(axis.text.x = element_text(size = rel(0.9)))
imp_plot
```

```{r, include = FALSE}
ggsave("imp_plot.pdf", imp_plot)
```

The plot suggests that the most "important" feature is `fbi_code`, which is somewhat unsurprising as this tells us what type of crime was reported. This would intuitively have a significant impact on whether the incident report led to a suspect being arrested. Note that none of the models achieved an accuracy greater than 90%. 

\pagebreak

# Prediction

In this section we consider the regression tesk of predicting the number of reported incidents of crime in a given region over a specified time period. We will look at a kernel ridge regression model before moving on to generalised additive models.

```{r, include = FALSE}
rm(list = ls())
```

## Assessing Performance

To assess performance, two metrics will be used: the root mean squared error (RMSE) and R-squared ($R^2$). These metrics can be computed using the functions `rmse_loss` and `r_squared`, respectively, from the **chigcrim** package.

```{r}
# List of metrics used to evaluate regression models
metrics <- list(rmse = rmse_loss, r2 = r_squared)
```

Repeated $k$-fold cross validation is used, and hyperparameters are chosen based on the RMSE metric. In particular, we will use 5 repetitions of 5-fold cross validation at each parameter value using the `kfold_cv` function.

\pagebreak

## Kernel Ridge Regression

The `R6` class `KernelRegression` fits a kernel ridge regression model with a specified kernel: one of `"linear"` (equivalent to ridge regression), `"polynomial"`, or `"radial"` (radial basis kernel).

As the relationship between the data and response is complex and non-linear, only the radial basis kernel will be considered here. This implicitly induces an extremely flexible infinite-dimensional feature transform. Note that we fix the regularisation parameter `lambda` as 0.001 in the ensuing analysis.

```{r, message = FALSE}
# Load required packages
library(rgdal)
library(sf)
library(ggmap)
# Use seed to ensure reproducibility
set.seed(1)
```

### Weekly Predictions

We first consider a simple, single-dimensional model, using only the number of crimes per week as the predictor.

```{r, include = FALSE}
df <- readRDS("temp_data/date_data.rds")
```

```{r, eval = FALSE}
df <- load_data(select = "date")
```

```{r}
df %<>% 
  mutate(date = date(date), week = week(date), year = year(date)) %>%
  filter(week < 53) %>%
  group_by(week, year) %>%
  mutate(week_start = min(date)) %>%
  ungroup() %>%
  count(week, week_start, year) %>%
  arrange(year, week) %>%
  mutate(cumulative_week = 1:nrow(.))
print(as.data.frame(head(df)))
```

We shall use a grid search with 5-fold cross validation, repeated 5 times to obtain a robust average for RMSE, in order to choose the bandwidth hyperparameter for the radial basis kernel.

```{r warning = FALSE}
X <- as.matrix(df$cumulative_week)
X <- scale(X)
y <- df$n
```

```{r, eval = FALSE}
# Candidate values for the bandwidth of the radial basis function
bandwidth <- c(0.1, 1, 5, 10, 100)
# Initisalise vector of RMSE
rmse_vec <- c()
# Conduct a grid search with 5-fold CV
for (i in 1:length(bandwidth)){
  kr = KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  cv_results <- kfold_cv(kr, X, y, metrics, k = 5, n_reps = 5, 
                         parallel = TRUE, n_threads = 5)
  mean_rmse <- mean(cv_results$rmse)
  rmse_vec <- c(rmse_vec, mean_rmse)
  # If mean RMSE is better than for the last best model clone and treat as best
  if (mean_rmse == min(rmse_vec)){
    # Clone the model and save
    kr_best <- kr$clone(deep = TRUE)
    # Update the best results
    cv_results_best <- cv_results
  }
}
# Save as data frame and plot results
rbf_results <- data.frame(bandwidth, rmse = rmse_vec)
bandwidth_plot <- ggplot(rbf_results) +
  geom_line(aes(x = bandwidth, y = rmse)) +
  scale_x_log10() +
  scale_y_log10() +
  xlab("Bandwidth") + ylab("RMSE")
bandwidth_plot
```

```{r, echo = FALSE, cache = TRUE, fig.height = 3}
cv_results_best <- readRDS("temp_data/cv_results_best1.rds")
kr_best <- readRDS("temp_data/kr_best1.rds")
bandwidth_plot <- readRDS("temp_data/cv_plot1.rds")
bandwidth_plot + xlab("Bandwidth") + ylab("RMSE")
```

We find that a bandwidth parameter value of `r kr_best$A` provided the lowest RMSE value. We can look at the results for the 5 repeats of 5-fold cross-validation for this model:

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(as_tibble(cv_results_best), digits = 4)
print(tab, type = "latex", comment = FALSE)
```

The model performs very well, explaining `r round(mean(cv_results_best$r2), 4)*100` % of the variance on average.

As this is single-dimensional, we can plot the prediction function for the model.

```{r}
# Fit the final kernel ridge regression model
kr_best$fit(X, y)
# Obtain predictions on the dataset
y_hat <- kr_best$predict(X)
# Plot predictions
df$y_hat <- y_hat
week_plot <- ggplot(df) + 
  geom_point(aes(x = week_start, y = n)) +
  geom_line(aes(x = week_start, y = y_hat), colour = "steelblue2", size = 1) +
  labs(x = "Year", y = "Number of reported incidents")
week_plot
```

The plot suggests a cyclic pattern over years, so naturally we proceed to consider using a trigonometric transformation of the data (taking year as the period), in the hope that we can capture this periodicity with a simpler model. Note that, whilst kernel ridge regression is useful for interpolation, this model with a radial basis kernel actually has limited utility for forecasting reported crime counts into the future. We illustrate this point by attempting to extrapolate from this model.

```{r, cache = TRUE}
# Get the start of the next week not included in the data
start_week_date <- max(df$week_start) + 7
# Get the corresponding week number
start_week_int <- max(df$cumulative_week) + 1
# Add the dates
extra_dates <- seq(ymd(start_week_date), ymd(start_week_date + 365*4),
                   by = '1 week')
extra_X <- start_week_int:(start_week_int + length(extra_dates) - 1)
extra_X <- (extra_X - attributes(X)$`scaled:center`) /
  attributes(X)$`scaled:scale`
# Predict using our fitted model
extra_y_hat <- kr_best$predict(as.matrix(extra_X))
# Attach these values to the data frame
extra_df <- tibble(extra_dates, extra_y_hat)
week_plot + geom_line(data = extra_df, aes(extra_dates, extra_y_hat), colour = "steelblue2")
```

The cycles are captured solely due to being able to interpolate locally between points. When extrapolating with a radial basis function kernel, no cycles are predicted, so the out-of-sample accuracy drops substantially. The cross-validation error found is only applicable if the new data followed the same distribution, and this is simply not the case when considering future data.

\pagebreak

### Monthly Predictions with a Spatial Component

The previous example aggregated counts of reported crime over weeks, and as such used only a single-dimensional feature vector. We look at extending this analysis by aggregating over an additional spatial component. In the ensuing analysis, we consider predicting the reported crime counts over a month, aggregated over the community areas.

The positions of the community areas will be represented by the coordinates of their centroids, which are extracted from their shapefiles. These shapefiles are included in the package for ease.

```{r, include = FALSE}
rm(list = ls())
```

```{r warning = FALSE, message = FALSE}
data("community_bounds")
centroids <- community_bounds %>%
  arrange(as.integer(area_numbe)) %>%
  st_centroid() %>%
  st_coordinates() %>%
  as_tibble() %>%
  mutate(community_area = 1:nrow(.))
# Plot the locations of centroids on map
centroids_plot <- ggplot() +
  geom_sf(data = community_bounds) +
  geom_point(data = centroids, aes(X, Y), size = 5, colour = "steelblue1") +
  geom_text(data = centroids, aes(X, Y, label = community_area)) +
  theme_void()
centroids_plot
```

We shall consider data from the years 2016 to 2020.

```{r, include = FALSE}
df <- readRDS("temp_data/data1620.rds")
```

```{r eval = FALSE}
df <- load_data(c(2016, 2020), strings_as_factors = FALSE, na_omit = TRUE)
```

```{r, message = FALSE}
df %<>% select(date, community_area) %>%
  mutate(date = date(date), year = year(date), month = month(date)) %>%
  group_by(year, month) %>%
  mutate(date = min(date)) %>%
  group_by_all() %>%
  summarise(n = n()) %>%
  left_join(centroids, by = "community_area")
```

In order to choose a suitable bandwidth parameter for the radial basis function, we again perform repeated 5-fold cross-validation.

```{r warning = FALSE}
# Obtain data matrix for fitting the model
X <- df %>% ungroup() %>%
  select(-community_area, -date, -n) %>%
  as.matrix()
# Scale the parameters
X <- scale(X)
y <- df$n
```

```{r, eval = FALSE}
# Bandwidth candidates
bandwidth <- c(0.1, 1, 5, 10, 100)
# Initialise vector for storing RMSE values
rmse_vec <- c()
# Set new seed for reproduciblility
set.seed(3)
for (i in 1:length(bandwidth)){
  kr <- KernelRidge$new("rbf", lambda = 0.001, bandwidth[i])
  cv_results <- kfold_cv(kr, X, y, metrics, k = 5, n_reps = 5,
                         parallel = TRUE, n_threads = 5)
  mean_rmse <- mean(cv_results$rmse)
  rmse_vec <- c(rmse_vec, mean_rmse)
  # Save best model
  if (mean_rmse == min(rmse_vec)){
    kr_best <- kr$clone(deep=TRUE)
    cv_results_best <- cv_results
  }
}
# Store as data frame and plot
rbf_results <- data.frame(bandwidth, rmse = rmse_vec)
band_plot <- ggplot(rbf_results) +
  geom_line(aes(x = bandwidth, y = rmse)) +
  scale_x_log10() +
  scale_y_log10()
band_plot
```

```{r, echo = FALSE, fig.height = 3}
cv_results_best <- readRDS("temp_data/cv_results_best2.rds")
kr_best <- readRDS("temp_data/kr_best2.rds")
bandwidth_plot <- readRDS("temp_data/cv_plot2.rds")
bandwidth_plot + xlab("Bandwidth") + ylab("RMSE")
```

We obtain an optimal value of `r kr_best$A` which minimises the mean square error. Again, we can look at the results for the three repeats of 5-fold cross-validation for this model:

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(as_tibble(cv_results_best))
print(tab, type = "latex", comment = FALSE)
```

On average, this model explained `r round(mean(cv_results_best$r2), 4)*100`% of the variance. Note the lower $R^2$ value: in contrast to the weekly model, observations were not averaged over the whole of Chicago.

We can plot the predicted values against the observed values on a held out data set to observe the predictive power of the model:

```{r, cache = TRUE}
# Randomly generate training and test data
index <- sample(1:nrow(X), round(0.2 * nrow(X)))
# Split the data
X_train <- X[-index, , drop = FALSE]
y_train <- y[-index]
X_test <- X[index, ]
y_test <- y[index]
# Refit best model on full training dataset
kr_best$fit(X_train, y_train)
y_hat <- kr_best$predict(X_test)
# Plot predicted values against actual number of reports
pred_plot <- qplot(y_test, y_hat, geom = "point") +
  geom_abline(slope = 1, intercept = 0, colour = "steelblue2") +
  labs(x = "Actual number of reported incidents", 
       y = "Predicted number of reported incidents") +
  scale_y_continuous(breaks = seq(0, 2000, by = 500))
pred_plot
```

```{r, include = FALSE}
ggsave("pred_plot.pdf", pred_plot)
```

The plot illustrates strong predictive power, particularly when the number of reported incidents is smaller. We do note the drop-off at the higher end of actual reported incidents, suggesting the model tends to under-predict in these cases. This is to be expected somewhat, as extremely high counts are rare and the model has not been trained on a dataset containing such high counts.

We conclude this section by noting that kernel ridge regression can provide prediction functions that explain a high proportion of the variance. However, a major limitation of this method in our case is that it would be primarily be useful for interpolation, rather than extrapolation, for the reasons outlined above. 

In the subsequent section, we explore generalised additive models, which form a more principled approach for forecasting the number of crimes.

\pagebreak

## Generalised Additive Models

Our `R6` class `PoissonGAM` is a wrapper 

A little about `PoissonGAM` and **mgcv**.

For the training data we will use historical data between 2015 and 2018. We will then predict on the data from 2019 to test the model.

```{r, include = FALSE}
rm(list = ls())
library(mgcv)
df_train <- readRDS("temp_data/data1518.rds")
df_test <- readRDS("temp_data/data19.rds")
```

```{r, eval = FALSE}
# Downoad training data
df_train <- load_data(c(2015, 2018), strings_as_factors = TRUE)
# Download test data
df_test <- load_data(2019, strings_as_factors = TRUE)
```

### Daily Predictions

```{r}
# Extract useful information from the date
df_train %<>% convert_dates(as_factors = FALSE, exclude = "hour")
df_test %<>% convert_dates(as_factors = FALSE, exclude = "hour")

# List of features to use in GAM
keep_features <- c("month", "week", "year", "day", "date", "community_area", 
                   "beat", "fbi_code", "yday")

# Prep data for use in GAM
gam_train <- df_train %>% select(all_of(keep_features)) %>% na.omit()
gam_test <- df_test %>% select(all_of(keep_features)) %>% na.omit()
```

As each row in the datasets represent a single reported crime incident, we need to sum the number of rows for each day in order to get the number of reported crimes for each day. We use `count` from **dplyr** to achieve this.

```{r}
count_train <- gam_train %>% count(yday, year, date) %>% arrange(year, yday)
count_test <- gam_test %>% count(yday, year, date) %>% arrange(year, yday)

print(as.data.frame(head(count_train)))
```

We separate this dataset into a training set and a test set, and proceed to fit our baseline GAM which has the year and day of the year as predictors. We use a cyclic cubic smoother for the day of the year. 

```{r, cache = TRUE}
# Convert data to matrix and vector format for model training
X_train <- count_train %>% select(-n)
y_train <- count_train$n
X_test <- count_test %>% select(-n)
y_test <- count_test$n

# Fit the baseline GAM
gam1 <- PoissonGAM$new(time_period = "yday", region = NULL, crime_type = NULL, 
                      include_nb = FALSE, include_year = TRUE)
gam1$fit(X = X_train, y = y_train, n_threads = 3)
gam1$fit_summary
pred <- gam1$predict(X = bind_rows(X_train, X_test))
# Store type of each data 
count_train$Type <- "Training data"
count_test$Type <- "Test data"
all_dat <- bind_rows(count_train, count_test)
all_dat$Type <- as.factor(all_dat$Type)

# Create plot for baseline model
base_gam_plot <- ggplot(all_dat) + 
  geom_point(aes(date, n)) + 
  xlab("Date") + ylab("Daily number of reported crimes") + 
  geom_line(aes(date, pred, colour = Type), size = 2) + 
  ggtitle("Baseline GAM") + 
  theme(legend.position = c(0.9, 0.9), 
        legend.background = element_rect(fill = "lightgrey", size = 0.5, 
                                         linetype = "solid"), 
        legend.title = element_text(size = 13), 
        legend.text = element_text(size = 10))
base_gam_plot

# Obtain metrics on test data
pred_test <- gam1$predict(X = X_test)
# Initialise vectors for RMSE and R squared values
rmse <- c()
rsq <- c()
rmse[1] <- rmse_loss(pred_test, y_test)
rsq[1] <- r_squared(pred_test, y_test)
```

### Feature creation

There are several features that may be of interest, whether a day is a weekend or not, the number of crimes over the previous month, the number of crimes on the same day of the previous year. 

We first summarise the dates on which the most and fewest crimes are reported, to gain some idea as to what factors might be influential.

```{r}
most_crimes <- count_train %>% arrange(by = desc(n))
most_crimes$date <- as.character(most_crimes$date)
```

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(head(most_crimes[, -5]), )
print(tab, type = "latex", comment = FALSE)
```

```{r}
least_crimes <- count_train %>% arrange(by = n)
least_crimes$date <- as.character(least_crimes$date)
```

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(head(least_crimes[, -5]), )
print(tab, type = "latex", comment = FALSE)
```

```{r}
# Add features to training data
count_train_new <- count_train %>% 
  mutate(week = week(date), month = month(date)) %>%
  add_prev_day() %>% add_dow() %>%  add_is_fom() %>% 
  add_is_christmas() %>% add_is_nyd()
# Add features to test data
count_test_new <- count_test %>%
  mutate(week = week(date), month = month(date)) %>%
  add_prev_day() %>% add_dow() %>%  add_is_fom() %>% 
  add_is_christmas() %>% add_is_nyd()

count_full <- bind_rows(count_train_new, count_test_new)
```

```{r, include = FALSE, cache = TRUE}
data_2014 <- readRDS("temp_data/data14.rds")
```

```{r, eval = FALSE}
data_2014 <- load_data(2014, strings_as_factors = TRUE)
```

```{r, cache = TRUE}
data_2014 <- data_2014 %>% 
  convert_dates(exclude = "hour") %>% 
  count(year, date) %>% 
  arrange(year)
# Fill in previous year manually
count_train_new[1, "n_pre"] <- data_2014[data_2014$date == "2014-12-31", "n"][1,] 
count_test_new[1, "n_pre"] <- count_train_new[count_train_new$date == "2018-12-31", "n"]
print(as.data.frame(head(count_train_new)))
```

As `PoissonGAM` doesn't currently have functionality to support additional custom features, here we fit the GAM directly using `gam` from **mgcv**.

```{r, cache = TRUE, warning = FALSE}
gam2 <- gam(n ~ s(as.numeric(yday), bs = "cc") + n_pre, 
            data = count_train_new, family = "poisson")
gam2_pred <- predict(gam2, type = "response", newdata = count_full)

gam2_plot <- ggplot(all_dat) + 
  geom_point(aes(date, n)) + 
  xlab("Date") + ylab("Daily number of reported crimes") + 
  geom_line(aes(date, gam2_pred, colour = Type), size = 2, alpha = 0.4) + 
  ggtitle("Daily GAM") + 
  theme(legend.position = c(0.9, 0.9), 
        legend.background = element_rect(fill = "lightgrey", size = 0.5, 
                                         linetype = "solid"), 
        legend.title = element_text(size = 13), 
        legend.text = element_text(size = 10))
gam2_plot
# Obtain metrics on test data
gam2_test <- predict(gam2, type = "response", newdata = count_test_new)
rmse[2] <- rmse_loss(gam2_test, count_test_new$n)
rsq[2] <- r_squared(gam2_test, count_test_new$n)
```

```{r, cache = TRUE, warning = FALSE}
gam3 <- gam(n ~ s(as.numeric(yday), bs = "cc") + 
              s(as.numeric(dow), bs = "cr", k = 5) + n_pre, 
            data = count_train_new, family = "poisson")
gam3_pred <- predict(gam3, type = "response", newdata = count_full)

gam3_plot <- ggplot(all_dat) + 
  geom_point(aes(date, n)) + 
  xlab("Date") + ylab("Daily number of reported crimes") + 
  geom_line(aes(date, gam3_pred, colour = Type), size = 2, alpha = 0.4) + 
  ggtitle("Daily GAM") + 
  theme(legend.position = c(0.9, 0.9), 
        legend.background = element_rect(fill = "lightgrey", size = 0.5, 
                                         linetype = "solid"), 
        legend.title = element_text(size = 13), 
        legend.text = element_text(size = 10))
gam3_plot
# Obtain metrics on test data
gam3_test <- predict(gam3, type = "response", newdata = count_test_new)
rmse[3] <- rmse_loss(gam3_test, count_test_new$n)
rsq[3] <- r_squared(gam3_test, count_test_new$n)
```

```{r, cache = TRUE, warning = FALSE}
# Fit GAM
gam4 <- gam(n ~ s(as.numeric(yday), bs = "cc") + 
              as.factor(dow) + n_pre, 
            data = count_train_new, family = "poisson")
# Predict on entire dataset
gam4_pred <- predict(gam4, type = "response", newdata = count_full)
# Plot predictions
gam4_plot <- ggplot(all_dat) + 
  geom_point(aes(date, n)) + 
  xlab("Date") + ylab("Daily number of reported crimes") + 
  geom_line(aes(date, gam4_pred, colour = Type), size = 2, alpha = 0.4) + 
  ggtitle("Daily GAM") + 
  theme(legend.position = c(0.9, 0.9), 
        legend.background = element_rect(fill = "lightgrey", size = 0.5, 
                                         linetype = "solid"), 
        legend.title = element_text(size = 13), 
        legend.text = element_text(size = 10))
gam4_plot
# Obtain metrics on test data
gam4_test <- predict(gam4, type = "response", newdata = count_test_new)
rmse[4] <- rmse_loss(gam4_test, count_test_new$n)
rsq[4] <- r_squared(gam4_test, count_test_new$n)
```

```{r, cache = TRUE, warning = FALSE}
gam5 <- gam(n ~ s(as.numeric(yday), bs = "cc") + as.factor(dow) + n_pre + 
              is_fom + is_christmas + is_nyd + as.factor(week), 
            data = count_train_new, family = "poisson")
# Predict on entire dataset
gam5_pred <- predict(gam5, type = "response", newdata = count_full)
# Plot predictions
gam5_plot <- ggplot(all_dat) + 
  geom_point(aes(date, n)) + 
  xlab("Date") + ylab("Daily number of reported crimes") + 
  geom_line(aes(date, gam5_pred, colour = Type), size = 2, alpha = 0.4) + 
  ggtitle("Daily GAM") + 
  theme(legend.position = c(0.9, 0.9), 
        legend.background = element_rect(fill = "lightgrey", size = 0.5, 
                                         linetype = "solid"), 
        legend.title = element_text(size = 13), 
        legend.text = element_text(size = 10))
gam5_plot
# Obtain metrics on test data
gam5_test <- predict(gam5, type = "response", newdata = count_test_new)
rmse[5] <- rmse_loss(gam5_test, count_test_new$n)
rsq[5] <- r_squared(gam5_test, count_test_new$n)
gam_metrics <- data.frame(RMSE = rmse, R2 = rsq)
```

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(gam_metrics)
print(tab, type = "latex", comment = FALSE)
```

### Adding Spatial Data

```{r, warning = FALSE}
# Construct training data set
spatial_train <- gam_train %>% 
  get_count_data(time_period = "month", region = "community_area")
# Construct test data set
spatial_test <- gam_test %>% 
  get_count_data(time_period = "month", region = "community_area")
# Split into data matrix and reponse vector
X_train <- spatial_train %>% select(-n)
y_train <- spatial_train$n
X_test <- spatial_test %>% select(-n)
metrics <- list(rmse = rmse_loss, r2 = r_squared)
```

```{r, include = FALSE}
mgam <- readRDS("temp_data/mGAM.rds")
mgam_results <- readRDS("temp_data/mGAMcv.rds")
```

```{r, eval = FALSE}
# Initialise PoissonGAM object
mgam <- PoissonGAM$new(time_period = "month", region = "community_area",
                     crime_type = NULL, include_nb = TRUE)

# Find CV error for given metrics
mgam_results <- kfold_cv(mgam, X = X_train, y = y_train, 
                         error_funcs = metrics, k = 5, n_reps = 5, 
                         parallel = TRUE, n_threads = 5)
```

```{r, cache = TRUE, results = "asis"}
names(mgam_results) <- c("RMSE", "R2")
tab <- xtable::xtable(as_tibble(mgam_results), digits = 3)
print(tab, type = "latex", comment = FALSE)
```

```{r, eval = FALSE}
# Re-fit model
mgam$fit(X_train, y_train, n_threads = 7)
```

```{r, cache = TRUE}
# Summary of the fit
mgam$fit_summary
# Predict on test data
spatial_test$predicted <- mgam$predict(X = X_test)
```

```{r, cache = TRUE, results = "asis"}
# Compute evaluation metrics
rmse <- rmse_loss(spatial_test$n, spatial_test$predicted)
rsq <- r_squared(spatial_test$n, spatial_test$predicted)
tab <- xtable::xtable(data.frame("RMSE" = rmse, "R2" = rsq))
print(tab, type = "latex", comment = FALSE)
```

```{r, cache = TRUE, message = FALSE}
data("community_bounds")
com_area <- spatial_test %>% 
  group_by(community_area) %>% 
  summarise(n = sum(n), n_pred = sum(predicted))
com_area$residual <- (com_area$n - com_area$n_pred)
community_bounds$residual <- com_area$residual

by_area <- ggplot(community_bounds) + geom_sf(aes(fill = residual)) + 
  ggtitle("Predicted Crime Frequency by Community Area") +
  colorspace::scale_fill_continuous_sequential(5, palette = "Inferno") + 
  theme_void() +
  guides(fill = guide_colorbar(title = "Frequency")) +
  scale_fill_gradientn(colors = colorspace::sequential_hcl(5, palette = "Inferno")[5:1], 
                       breaks = seq(-1500, 1000, 500), limits = c(-1500, 1000))
by_area
```

### Adding Categorical Data

```{r, cache = TRUE}
# Construct training data set
spatial_fbi_train <- gam_train %>% 
  get_count_data(time_period = "month", region = "community_area", 
                 crime_type = "fbi_code")
# Construct test data set
spatial_fbi_test <- gam_test %>% 
  get_count_data(time_period = "month", region = "community_area", 
                 crime_type = "fbi_code")
# Split into data matrix and reponse vector
X_train <- spatial_fbi_train %>% select(-n)
y_train <- spatial_fbi_train$n
X_test <- spatial_fbi_test %>% select(-n)
```

```{r, eval = FALSE}
# Initisalise PoissonGAM
mgam_com_fbi <- PoissonGAM$new(time_period = "month", region = "community_area",
                     crime_type = "fbi_code", include_nb = FALSE)
# Find CV error for given metrics
mgam_fbi_results <- kfold_cv(mgam_com_fbi, X = X, y = y, error_funcs = metrics, 
                        k = 5, n_reps = 5, parallel = TRUE, n_threads = 5)
names(mgam_fbi_results) <- c("RMSE", "R2")
```

```{r, include = FALSE}
mgam_fbi_results <- readRDS("temp_data/GAM_results.rds")
names(mgam_fbi_results) <- c("RMSE", "R2")
mgam_com_fbi <- readRDS("temp_data/wGAM.rds")
```

```{r, cache = TRUE, results = "asis"}
tab <- xtable::xtable(as_tibble(mgam_fbi_results), digits = 4)
print(tab, type = "latex", comment = FALSE)
```

```{r, eval = FALSE}
# Fit full model
mgam_com_fbi$fit(X_train, y_train, n_threads = 7)
```

```{r, cache = TRUE, message = FALSE}
spatial_fbi_test$predicted <- mgam_com_fbi$predict(X = X_test)
```

```{r, cache = TRUE, results = "asis"}
# Compute evaluation metrics
rmse <- rmse_loss(spatial_fbi_test$n, spatial_fbi_test$predicted)
rsq <- r_squared(spatial_fbi_test$n, spatial_fbi_test$predicted)
tab <- xtable::xtable(data.frame("RMSE" = rmse, "R2" = rsq))
print(tab, type = "latex", comment = FALSE)
```

```{r, cache = TRUE, message = FALSE, warning = FALSE}
com_area <- spatial_fbi_test %>% group_by(community_area) %>% 
  summarise(n = sum(n), n_pred = sum(predicted))
com_area$residual <- (com_area$n - com_area$n_pred)
community_bounds$residual <- com_area$residual

# Plot the residuals for each Community Area
by_area <- ggplot(community_bounds) + 
  geom_sf(aes(fill = residual)) + 
  ggtitle("Predicted Crime frequency by Community Area") +
  colorspace::scale_fill_continuous_sequential(5, palette = "Inferno") + 
  theme_void() +
  guides(fill = guide_colorbar(title = "Frequency")) +
  scale_fill_gradientn(colors = colorspace::sequential_hcl(5, palette = "Inferno")[5:1], 
                       breaks = seq(-1500, 1000, 500), limits = c(-1500, 1000))
by_area
```